[{"title":"理解各种各样的CNN架构","date":"2019-04-15T13:27:37.000Z","path":"2019/04/15/理解各种各样的CNN架构/","text":"ResNet-Alexnet-VGG-Inception:理解各种各样的CNN架构本文翻译自ResNet、AlexNet、VGG、Inception：Understanding various architecture of Convelutional Neural Network ResNet、AlexNet、VGG、Inception：Understanding various architecture of Convelutional Neural Network 原作者保留版权。 卷积神经网络（CNN）在视觉识别任务上的表现令人惊奇。好的CNN网络是带有上百万参数和寻多隐藏层的“庞然怪物”。事实上，一个不好的经验规则是：网络越深，效果越好。要打破这种认知 AlexNet、VGG、Inception和ResNet都是最近一些流行的CNN网络。 为什么这些网络表现如此只好？它们又是如何设计出来的？为什么他们设计成那样的结构？回答这些问题并不简单，但是这里我们试着去探讨上面的几个问题。 网络结构设计是一个复杂的过程，需要花很多时间 为什么CNN模型战胜了传统计算机视觉方法？图像分类指的是：给定一个图片，将其分类成预定义好的几个类别之一。图像分类的传统流程设计两个模块：特征提取和分类 特征提取指的是从原始像素提取更高级的特征，这些特征能够捕捉到类别之间的区别。这种特征提取的方式是使用无监督方式，从像素点中提取信息时，没有使用图像的类别标。 常用的传统特征包括GIST、HOG、SIFT、LBP等。特征提取后，使用图像的这些特征与其对应的类别标签训练一个分类模型。常用的分类模型有SVM、LR、随机森林及决策树等。 上面流程的一大问题是： 特征提取不能根据图像及其标签进行调整。如果选择的特征缺乏一定的代表性来区分各个类别，模型的准确度就会大打折扣，而无论采取什么样的分类策略。 采用传统的流程，目前的一个比较好的方法是使用多种特征提取器，然后组合他们得到一种更好的特征（组合特征）。 但是这需要很多启发式规则和人力来根据领域的不同来调整参数，使得达到一个很好的准确度，这里说的是要接近人类的水平。 这也就是说，为什么采用传统的计算机视觉技术需要花费很多年时间才能打造出一个好的计算机视觉系统（例如：OCR、人脸验证、图像识别、物体检测），浙西系统在实际应用中可以处理各种数据。 有一次，我们用了6周时间为一家公司打造了一个CNN模型，其效果更好，如果采用传统的计算机视觉技术要达到同样效果则要花费一年时间。 传统流程的另一个问题是： 它与人类识别学习物体的过程是完全不一样的。人自出生之初，一个孩子就可以感知周围环境，随着他的成长，他接触的数据更多，从而学会了识别物体。 这是深度学习背后的哲学，其中并没有建立硬编码的特征提取器。它将特征提取和分类两个模块集成为一个系统，通过识别图像的特征来进行提取并基于有标签的数据进行分类。 这样的集成系统就是多层感知机。即有多层神经元密集连接而成的神经网络。 一个经典的深度网络包含很多参数，由于缺乏足够的训练样本，基本不可能训练处一个不过拟合的模型。 但是，对于CNN模型，从头开始训练一个网络时，你可以使用一个很大的数据集，比如ImageNet。这背后的原因是CNN模型的两个特点：神经元间的权重共享和卷积层之间的稀疏连接。 这可以从下图看出，在卷积层，某一个层的神经元只是和输入层的神经元局部连接，而且卷积核的参数时在整个2-D特征图上共享的。 为了理解CNN背后的设计哲学，你可能会问：其目标是什么？（1）准确度 如果你在搭建一个智能系统，最重要的当然是要尽可能的准确。公平的来说，准确度不仅取决于网络，也取决于训练样本的数量。因此，CNN模型一般在一个标准数据集ImageNet上做对比。 ImageNet项目仍然在继续改进，目前已经有包含21,841类的14,197,122个图片。自从2010年，每年都会举行ImageNet图像识别竞赛，比赛会提供从ImageNet数据集中抽取属于1000类的120万中图片。 每个网络架构都是在这120W张图片上测试其在1000分类上的准确度。 （2）计算量 大部分CNN模型都需要很大的内存和计算量，特别是在训练过程中。因此，计算量会成为一个重要的关注点。同样的，如果你想部署在移动端，训练得到的最终模型大小也需要特别考虑。 你可以想象到，为了得到更好的准确度，你需要一个计算更密集的网络，因此，准确度和计算量需要折中。 除了上面两个因素，还有其他需要考虑的因素，比如训练的容易度，模型的泛化能力等。下面按照提出时间，介绍一些流行的CNN架构，可以看到他们的准确度越来越高。 AlexNetAlexNet AlexNet是一个较早应用于ImageNet上的深度网络，其准确度相比传统方法有一个很大的提升。它首先是5个卷积层，然后紧跟着是3个全连接层，如下图所示： Alex Krizhevs提出的AlexNet首次采用了Relu激活函数，而不像传统神经网络早期所采用的Tanh或Sigmoid激活函数，ReLU数学表达式为：$f(x)=max(0,x)$ ReLU相比Sigmoid的优势是其训练速度更快，因为Sigmoid的导数在稳定区会非常小，从而其权重基本不再更新。这就是梯度消失问题。因此，AlexNet在卷积层和全连接层后面使用了ReLu激活函数。 AlexNet的另一个特点是其通过在每个全连接层后面加了Dropout层减少了模型的过拟合问题。Dropout层以一定的概率随机地关闭当前层中神经元激活值，如下图所示： 为什么Dropout有效？ Dropout背后的理念和集成模型很相似。在Dropout层，不同的神经元组合被关闭，这代表了一种不同的结构，所有这些不同的结构都是与每个子集的权值并行训练的，权值之和为1. 如果Dropout层有n个神经元，那么会形成$2^n$个不同的子结构，在预测时，相当于集成这些模型并取均值，这种结构化的模型正则化技术有利于避免过拟合。 Dropout有效的另一个观点是：由于神经元是随机选择的，所以可以减少神经元之间的相互依赖，从而确保提取出相互独立的重要特征。 AlexNet代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimimport torchvisionimport torchvision.transforms as transforms# import torchvision.datasets as datasetsimport osimport randomimport numpy as npclass AlexNet(nn.Module): def __init__(self): #init函数定义的是网络的架构、关键的网络模块、模组 super(AlexNet,self).__init__() self.feature_block=nn.Sequential( # Conv2d位置参数：in_channels，out_channels nn.Conv2d(3,64,kernel_size=11,stride=4,padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3,stride=2), nn.Conv2d(64,192,kernel_size=5,padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3,stride=2), nn.Conv2d(192,384,kernel_size=3,padding=1), nn.ReLU(inplace=True), nn.Conv2d(384,256,kernel_size=3,padding=1), nn.ReLU(inplace=True), nn.Conv2d(256,256,kernel_size=3,padding=1), nn.ReLU(inplace=True) ) #5个连续的卷积层 self.avgpool=nn.AdaptiveAvgPool2d((6,6)) self.class_block=nn.Sequential( nn.Dropout() nn.Linear(256*6*6,4096), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(4096,4096), nn.ReLU(inplace=True), nn.Linear(4096,10) ) def forward(self,x): x=self.feature_block(x) x=self.avgpool(x) x=x.view(x.size[0],256*6*6) x=self.class_block(x) return VGG-16VGG-16 VGG-16是牛津大学VGG组提出的。VGG16相比AlexNet的一个改进是采用连续的几个3*3卷积核代替AlexNet中较大的卷积核（11*11,5*5）。 对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核是优于采用大的卷积核，因为多层非线性层可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。 比如，3个步长为1的3×3卷积核连续作用在一个大小为7的感受野，其参数总量为3×(3^3×C^2),如果直接使用7×7卷积核，其参数总量为7^7×C^2,这里的C值得是出入和输出的通道数。 而且3×3卷积核有利于更好地保持图像性质。VGG网络的架构如下所表示： 可以看到VGG使用了一种块结构，多次重复使用同一大小的卷积核来提取更复杂和更具有表达性的特征。这种块（blocks/modules）在VGG之后被广泛采用。 VGG卷积层之后是3个全连接层。网络的通道数从较小的64开始，然后每经过一个downsample或者池化层成倍地增加，当然特征图大小成倍地减小。最终其在ImageNet上的Top-5准确度为92.3% VGG代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768class VGGBlock(nn.Module): def __init__(self,in_channels,out_channels,batch_norm): #在后来，改良后的VGG网络增加了BatchNorm super(VGGBlock,self).__init__() stack=[] stack.append(nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1)) if batch_norm: stack.append(nn.BatchBorm2d(out_channels)) stack.append(nn.ReLU(inplace=True)) self.model_block=nn.Sequential(*stack) def forward(self,x): return self.model_block(x) class VGG11(nn.Module): def __init__(self,block,pool,batch_norm): self.feature_block=nn.Sequential( block(3,64,batch_norm), #32*32个 pool(kernel_size=2,stride=2), #16*16 block(64,128,batch_norm), pool(kernel_size=2,stride=2) #8*8 block(126,256,batch_norm), block(256,256,batch_norm), pool(kernel_size=2,stride=2), #4*4 block(256,512,batch_norm), block(512,512,batch_norm), pool(kernel_size=2,stride=2), #2*2 block(512,512,batch_norm), block(512,512,batch_borm), pool(kernel_size=2,stride=2), #1*1 ) self.classifier=nn.Linear(512,10) def forward(self,x): x=self.feature_block(x) x=x.view(x.size[0],-1) x=self.classifier(x) return class VGG16(nn.Module): def __init__(self,block,pool,batch_norm): super(VGG16,self).__init__() self.feature_block=nn.Sequential( block(3,64,batch_norm), block(64,64,batch_norm), pool(kernel_size=2,stride=2), block(64,128,batch_norm), block(128,128,batch_norm), pool(kernel_size=2,stride=2), block(128,256,batch_norm), block(256,256,batch_norm), block(256,256,batch_norm), pool(kernel_size=2,stride=2), block(256,512,batch_norm), block(512,512,batch_norm), block(512,512,batch_norm), pool(kernel_size=2,stride=2), block(512,512,batch_norm), block(512,512,batch_norm), block(512,512,batch_norm), pool(kernel_size=2,stride=2), ) self.classifier=nn.Linear(512,10) def forward(self,x): x=self.feature_block(x) x=x.view(x.size[0],-1) x=self.classifier(x) return x GoogLeNet/Inception尽管VGG可以在ImageNet上表现很好，但是将其部署在一个适度大小的GPU上是困难的，因为需要VGG在内存和时间上的计算要求都很高。由于卷积层的通道数过大，VGG并不高效。 比如，在一个3×3的卷积核，如果其输入和输出的通道数均为512，那么需要的计算量为9×512×512. 在卷积操作中，输出特征图上某一个位置，其是与所有的输入特征图是相连的，这是一种密集连接结构。 GoogLeNet 基于这样的理念：在深度网络中大部分的激活值是不必要的（为0），或者由于相关性是冗余。因此，最高效的深度网络架构应该是激活值之间是稀疏连接的，这就意味着512个输出特征图是没必要与所有的512输入特征图相连。 存在一些技术可以对网络进行剪枝来得到稀疏权重或者连接。但是稀疏卷积核的乘法在BLAS和CuBlas中并没有优化，这反而造成稀疏连接结构比密集结构更慢。 据此，GoogLeNet设计了一种称为inception的模块，这个模块使用密集结构来近似一个稀疏的CNN，如下图所示。 前面说过，只有很少一部分神经元是真正有效的，所以一种特定大小的卷积核数量设置地非常小。同时，GoogLeNet使用了不同大小的卷积核来抓取不同大小的感受野。 Inception模块的另一个特点是使用了一种瓶颈层（事实上就是1×1卷积）来降低计算量： 这里假定Inception模块的输入为192通道，它使用128个3×3卷积核和32个5×5卷积核。5×5卷积的计算量为25×32×192，但是随着网络变深，网络的通道数和卷积核数会增加，此时计算量就暴涨了。为了避免这个问题，在使用较大卷积核之前，先去降低通道数。 所以，Inception模块首先送入只有16个1×1层卷积层，然后再送给5×5卷积层。这样整体计算量会减少为16×192+25×32×16。这种设计允许网络可以使用更大的通道数。（译者注：之所以称1×1卷集层为瓶颈层，可以想象一下一个1×1卷积层拥有最少的通道数，这在Inception模块就像一个瓶子的最窄处。） GoogLeNet的另一个特殊设计是最后的卷积层后使用全局均值池化层替换了全连接层，所谓全局池化就是在整个2D特征图上取均值。 这大大减少了模型的总参数量。要知道在AlexNet中，全连接层参数占到整个网络总参数的90%，使用一个更大更深的网络使GoogLeNet移除全连接层之后还不影响准确度。其在ImageNet上的top-5准确度为93.3%，但是速度还比VGG快。 GoogLeNet代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980class Inception(nn.Module): def __init__(self,in_planes,n1x1,n3x3red,n3x3,n5x5red,n5x5,pool_planes): super(Inception,self).__init__() self.b1=nn.Sequential( nn.Conv2d(in_planes,n1x1,kernel_size=1), nn.BatchNorm2d(in_planes), nn.ReLU(True), ) self.b2=nn.Sequential( nn.Conv2d(in_planes,n3x3red,kernel_size=1), nn.BatchNorm2d(n3x3red), nn.ReLU(True), nn.Conv2d(n3x3red,n3x3,kernel_size=3,padding=1), nn.BatchNorm2d(n3x3), nn.ReLU(True), ) self.b3=nn.Sequential( nn.Conv2d(in_planes,n5x5red,kernel_size=1), nn.BatchNorm2d(n5x5red), nn.ReLU(True), nn.Conv2d(n5x5red,n5x5,kernel_size=5,padding=2), nn.BatchNorm2d(n5x5), nn.ReLU(True), ) self.b4=nn.Sequential( nn.MaxPool2d(3,stride=1,padding=1), nn.Conv2d(in_planes,pool_planes,kernel_size=1), nn.BatchNorm2d(pool_planes), nn.ReLU(True), ) def forward(self,x): x1=self.b1(x) x2=self.b2(x) x3=self.b3(x) x4=self.b4(x) # concat 4层输入到一起 return torch.cat([x1,x2,x3,x4],1) class GoogLeNet(nn.Module): def __init__(self): super(GoogLeNet,self).__init__() self.feature_block=nn.Sequential( nn.Conv2d(1,192,kernel_size=3,padding=1), nn.BatchNorm2d(192), nn.ReLU(True), ) self.a3=Inception(192,64,96,128,16,32,32) self.b3=Inception(256,128,128,192,32,96,64) self.maxpool=nn.MaxPool2d(3,stride=2,padding=1) self.a4=Inception(480,192,96,208,16,48,64) self.b4=Inception(512,160,112,224,24,64,64) self.c4=Inception(512,128,128,256,24,64,64) self.d4=Inception(512,112,144,288,32,64,64) self.e4=Inception(528,256,160,320,32,128,128) self.a5=Inception(832,256,160,320,32,128,128) self.b5=Inception(832,384,192,384,48,128,128) self.avgpool=nn.AvgPool2d(8,stride=1) self.linear=nn.Linear(1024,10) def forward(self,x): out=self.feature_block(x) out=self.a3(out) out=self.b3(out) out=self.maxpool(out) out=self.a4(out) out=self.b4(out) out=self.c4(out) out=self.d4(out) out=self.e4(out) out=self.maxpool(out) out=self.a5(out) out=self.b5(out) out=self.avgpool(out) out=out.view(out.size(0),-1) out=self.linear(out) return out ResNet从前面可以看到，随着网络深度增加，网络的准确度应该同步增加，当然要注意过拟合问题。但网络深度增加的一个问题在于这些增加的层是参数更新的信号，因为梯度是从后向前传播的，增加网络深度后，比较靠前的层的梯度会很小。 这意味着这些层基本上学习停滞了，这就是梯度消失问题。深度网络的第二个问题在于训练，当网络更深时，意味着参数空间更大，优化问题变得更难，因此简单的去增加网络深度反而出现更高的训练误差。 残差网络【ResNet】 设计一种残差模块，让我们可以训练更深的网络 为什么残差容易训练？ $y_l=h(x_l)+F(x_l,W_l)$ $x_{l+1}=f(y_l)$ $h(x_l)=x_l$ $x_L=x_l+\\sum_{i=l}^{L-1}F(x_i,W_i)$ 其中：$F$是残差函数 || $h$是恒等变换 || $x_L$是融合后的函数 || $f$是激活函数 $\\cfrac{\\partial loss}{\\partial x_l}=\\cfrac{\\partial loss}{\\partial x_L}·\\cfrac{\\partial x_L}{\\partial x_l}=\\cfrac{\\partial loss}{\\partial x_L}·\\left( 1 + \\cfrac{\\partial}{\\partial x_L}\\sum_{i=l}^{L-1}F(x_i,W_i)\\right)$ 深度网络的训练问题称之为退化问题，残差单元可以解决退化问题的背后逻辑在于此：想象一个网络A，其训练误差为x。现在通过A上面堆积更多的层来构建网络B，这些新增的层什么也不做，仅仅复制前面A的输出，这些新增层称之为C。 这意味着，网络B应该和A的训练误差一样，那么训练网络B其训练误差应该不会差于A。但是实际上却是更差，唯一的原因是，让增加的层C学习恒等映射并不容易。为了解决这个退化问题，残差模块在输入和输出特征之间建立了一个直接连接，这样新增的层C仅仅需要在原来的输入层基础上学习新特征，即学习残差，会比较容易。 与GoogLeNet类似，ResNet也在最后使用了全局均值池化层。利用残差模块，可以训练152层的残差网络，其准确度比VGG和GoogLeNet要高，但是计算效率也比VGG高，152层的ResNet其Top-5准确度为95.51% ResNet主要使用3×3卷积，这与VGG类似，在VGG基础上，短路连接插入形成残差网络。如图所示： 残差网络实验结果表明：34层的普通网络比18层网络训练误差还大，这就是前面所说的退化问题。但是34层的残差网络比18层残差网络训练误差要好。 ResNet代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class ResNetBlock(nn.Module): def __init__(self,in_channels,out_channels,stride): super(ResNetBlock,self).__init__() self.conv1=nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=stride,padding=1,bias=False) self.bn1=nn.BatchNorm2d(out_channels) self.conv2=nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=1,padding=1,bias=False) self.bn2=nn.BatchNorm2d(out_channels) self.downsample=nn.Sequential() if stride!=1 or in_channels!=out_channels: self.downsample=nn.Sequential( nn.Conv2d(in_channels,out_channels,kernel_size=1,strid=stride,bias=False), nn.BatchNorm2d(out_channels) ) def forward(self,x): out=F.relu(self.bn1(self.conv1(x))) out=self.bn2(self.conv2(out)) out+=self.downsample(x) #ResNet的add操作，其实是张量的加和 out=F.relu(out) return out class ResNetLayer(nn.Module): def __init__(self,block,n_blocks,in_channels,out_channels,stride): super(ResNetLayer,self).__init__() self.modules=[] self.modules.append(block(in_channels,out_channels,stride)) for _ in range(n_blocks-1): self.modules.append(block(out_channels,out_channels,1)) self.blocks=nn.Sequential(*self.modules) def forward(self,x): return self.blocks(x) class ResNet18(nn.Module): def __init__(self,layer,block): super(ResNet18,self).__init__() n_blocks=[2,2,2,2] self.conv1=nn.Conv2d(3,64,kernel_size=2,stride=1,padding=1,bias=False) self.bn1=nn.BatchNorm2d(64) self.rb1=layer(block,n_blocks[0],64,64,1) self.rb2=layer(block,n_blocks[1],64,128,2) self.rb3=layer(block,n_blocks[2],128,256,2) self.rb4=layer(block,n_blocks[3],256,512,2) self.fc=nn.Linear(512,10) def forward(self,x): out=F.relu(self.bn1(self.conv1(x))) out=self.rb1(out) out=self.rb2(out) out=self.rb3(out) out=self.rb4(out) out=F.avg_pool2d(out,4) out=out.view(out.shape[0],-1) out=self.fc(out) return out DenseNetDenseNet是一种具有密集连接的卷积神经网络，即：任何两层之间都有直接的连接。也就是说，网络每一层的输入都是对前面所有层输出的并集，而该层所学习的特征图也会被直接传给其后所有层作为输入。这种密集连接，通过将feature在channel上的连接来实现特征重用（feature reuse），这让DenseNet在参数和计算成本更少的情形下实现比ResNet更优的性能，DenseNet也因此斩获CVPR2017的最佳论文奖。 章节目录 密集连接机制 网络组成架构 DenseBlock和Transition的细节 DenseNet的优点 FC-DenseNet语义分割 密集连接机制即相互连接所有的层，具体来说就是每个层都会接受前面所有层作为其额外的输入，在DenseNet中，每个层都会与前面所有层在Channel维度上连接（conact）在一起，并作为下一层的输入。对于一个L层网络，DenseNet共包含L(L+1)/2个连接。DenseNet是直接concat来自不同层的特征图来实现特征重用。 网络组成架构由于密集连接的时候要求特征图大小是一致的，为了随着网络深度的加深，特征图大小也降低，也就是使用pooling，使用DenseBlock和Transition两种模块来组合。DenseBlock模块里面层与层之间采用密集连接方式，Transition连接两个相邻的DenseBlock，使用Pooling降低分辨率。看图： DenseBlock和Transition的细节 非线性组合： DenseBlock中非线性组合函数，比如：BN+ReLU+3x3 Conv结构，而且各个层的特征图大小是一致的，这样才能保证channel维度上的concat。 增长率（Growth Rate） 由于网络中每一层都直接与其它层相连接，实现特征的重复利用，因此为了降低冗余性，使用了一个超参K，用于控制每一层输出的feature maps厚度，那一般情况下使用较小的K（比如12），目睹是为了把网络的每一层设计的特别【窄】，即只学习非常少的特征图（最极端的情况是每一层只学习一个特征图）。假定输入层的特征图的channel数为3，那么L层输入的channel数为3+K(L-1),因此随着层数增加，尽管k设定的较小，DenseBlock的输入会非常多，不过这是由于特征重用造成的，每个层仅有K个特征是自己独有的，使用K个卷积核对（x0,x1,x2,x3）组成的特征图进行非线性变换得到通道数为K的x4,再通过x4和前面每个阶段的特征图拼接起来得到（x0,x1,x2,x3,x4） bottleneck层 虽然k使用了较小的值，但是通过多个层不断的密集连接，最终特征图的通道数还是会比较大而造成网络计算量增大。因此在非线性变换的时候引入了1x1的卷积来进行降维，引入1x1卷积之后的非线性变换变成了BN+ReLU+1x1Conv+BN+ReLU+3x3Conv，称之为DenseNet-B结构，看图： Transition层： Transition层也是非线性变换函数组成的模块，组合方式为：BN+ReLU+1x1Conv+2x2Pooling，既然是非线性变换组合，在这个组合中加入pooling就可以起到降低特征图大小的作用，那么它可以通过控制输入的卷积核个数，起到进一步压缩特征图的作用。假设一个DenseBlock中包含m个特征图，那么我们使其输出连接的transition layer层生成$\\theta m$个输出特征图。当这里的$\\theta$取值为(0,1]时，起到压缩的作用，因此可以叫做压缩系数，当$\\theta =1$时，transition layer将保留原特征feature维度不变。 DenseNet优点(1)省参数，在ImageNet分类数据集上达到相同的准确率，在DenseNet上所需参数量不到ResNet一半。 (2)省计算，达到与ResNet相当的精度，DenseNet所需计算量也只有ResNet的一半左右。 (3)抗过拟合，对于DenseNet抗过拟合的原因有一个比较直观的解释：神经网络每一层提取特征都相当于对输入数据的一个非线性变换，随着深度的增加，变换的复杂度也逐渐增加(更多非线性函数的组合)。相比于一般神经网络直接依赖于网络最后一层（复杂度最高）的特征，DenseNet可以综合利用前层复杂度低的特征，因而更容易得到一个光滑的具有更好泛化性能的决策函数。 (4)支持特征重用，强化特征传播，有效解决梯度消失问题。 FC-DenseNet语义分割参考 由上图可以看出，全卷机DenseNet使用DenseNet作为它的基础编码器，并且也以类似于U-Net的方式，在每一层级上将编码器和解码器进行拼接。在解码阶段，将卷及操作替换为dense模块，并由transition up模块完成升采样，transition up模块使用的转置卷积升采样以前的特征图，然后将升采样后的特征图连接到来自下采样过程中的dense模块的跨层连接。需要注意的是，为了解决特征图数目的线性增长问题，dense模块的输入并不连接到它的输出，转置卷积仅对最后一个dense block的特征图使用。因为最后一个dense模块包含了前面虽有部分分辨率dense模块信息之和。同时引入跳层解决之前dense block特征损失的问题。网络结构如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import mathclass Bottleneck(nn.Module): def __init__(self,in_planes,growth_rate): super(Bottleneck,self).__init__() self.bn1=nn.BatchNorm2d(in_planes) self.conv1=nn.Conv2d(in_planes,4*growth_rate,kernel_size=1,bias=False) self.bn2=nn.BatchNorm2d(4*growth_rate) self.conv2=nn.Conv2d(4*growth_rate,growth_rate,kernel_size=3,padding=1,bias=False) def forward(self,x): out=self.conv1(F.relu(self.bn1(x)))#pre-activation out=self.conv2(F.relu(self.bn2(out))) out=torch.cat([out,x],1) return outclass Transition(nn.Module): def __init__(self,in_planes,out_planes): super(Transition,self).__init__() self.bn=nn.BatchNorm2d(in_planes) self.conv=nn.Conv2d(in_planes,out_planes,kernel_size=1,bias=False) def forward(self,x): out=self.conv(F.relu(self.bn(x))) out=F.avg_pool2d(out,2) return outclass DenseNet(nn.Module): def __init__(self,block,nblocks,growth_rate=12,reduction=0.5,num_classes=10): super(DenseNet,self).__init__() self.growth_rate=growth_rate num_planes=2*growth_rate #32 #最初的感知层 self.conv1=nn.Conv2d(3,num_planes,kernel_size=3,padding=1,bias=False) #第一个DenseBlock self.dense1=self._make_dense_layers(block,num_planes,nblocks[0]) num_planes+=nblocks[0]*growth_rate out_planes=int(math.floor(num_planes*reduction)) self.trans1=Transition(num_planes,out_planes) num_planes=out_planes #第二个DenseBlock self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1]) num_planes += nblocks[1]*growth_rate#计算如果不压缩的话的输出 out_planes = int(math.floor(num_planes*reduction)) self.trans2 = Transition(num_planes, out_planes) num_planes = out_planes #第三个DenseBlock self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2]) num_planes += nblocks[2]*growth_rate out_planes = int(math.floor(num_planes*reduction)) self.trans3 = Transition(num_planes, out_planes) num_planes = out_planes #第四个DenseBlock self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3]) num_planes += nblocks[3]*growth_rate #分类层 self.bn=nn.BatchNorm2d(num_planes) self.linear=nn.Linear(num_planes,num_classes) def _make_dense_layers(self,block,in_planes,nblock): #block:bottleneck #nblock代表构建denseblock中有多少bottleneck层 layers=[] for i in range(nblock): layers.append(block(in_planes,self.growth_rate)) in_planes+=self.growth_rate return nn.Sequential(*layers) def forward(self,x): out=self.conv1(x) out=self.trans1(self.dense1(out)) out = self.trans2(self.dense2(out)) out = self.trans3(self.dense3(out)) out = self.dense4(out) out=F.avg_pool2d(F.relu(self.bn(out)),4) out=out.view(out.size(0),-1) out=self.linear(out) return outdef DenseNet121(): return DenseNet(Bottleneck,[6,12,24,16],growth_rate=32)","tags":[{"name":"CNN arch deeplearning","slug":"CNN-arch-deeplearning","permalink":"http://yoursite.com/tags/CNN-arch-deeplearning/"}]},{"title":"Hello World","date":"2018-07-11T13:22:02.774Z","path":"2018/07/11/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]